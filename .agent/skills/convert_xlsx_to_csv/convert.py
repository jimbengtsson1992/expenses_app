import pandas as pd
import sys
import os

def convert_xlsx_to_csv(input_path, output_path, merge_source=None):
    try:
        # Read the Excel file without assuming a header, to preserve original structure
        # We need the metadata from the top, but for the dataframe we want the actual data
        # which starts at row 4 (index 3)
        df_new = pd.read_excel(input_path, header=3, engine='openpyxl')
        
        # Function to format dates
        def format_date(x):
            if isinstance(x, pd.Timestamp):
                return x.strftime('%Y-%m-%d')
            return x

        # Apply date formatting
        # Note: map works on Series (columns), applymap on DataFrame. 
        # In newer pandas, map can be used on DataFrame but it's safer to target specific columns if possible,
        # or use applymap for older pandas compatibility if needed. However, since we saw 'df.map' in previous
        # view, we stick to what works or standard pandas.
        # Let's perform date formatting securely on the 'Datum' and 'Bokfört' columns if they exist,
        # or generally on object columns that look like dates. 
        # For simplicity and sticking to previous logic:
        df_new = df_new.map(format_date)
        
        # Helper to clean and normalize dataframe
        def clean_df(df):
             if 'Datum' not in df.columns:
                 return df
             
             # Filter garbage rows
             df = df.dropna(subset=['Datum'])
             df = df[~df['Datum'].astype(str).str.contains('Datum|Summa|Transaktionsexport|Totalt', case=False, na=False)]
             
             # Normalize Datum
             # We want yyyy-MM-dd string for final output, but for deduplication datetime is safer,
             # OR just consistent string. Since we format df_new to string earlier, let's stick to string.
             # df_new['Datum'] is already formatted string.
             # df_old['Datum'] might be anything.
             # Let's try to convert to datetime then back to string to ensure canonical format
             df['Datum'] = pd.to_datetime(df['Datum'], errors='coerce').dt.strftime('%Y-%m-%d')
             df = df.dropna(subset=['Datum']) # Drop rows that failed date parsing
             
             # Normalize Belopp (Amount)
             # Should be 'Belopp' column. Check if it exists.
             if 'Belopp' in df.columns:
                 # Remove spaces, comma to dot if needed (though usually read properly if configured)
                 # CSV usually has decimal points if generated by this script.
                 # But just in case:
                 def clean_amount(x):
                     if isinstance(x, str):
                         # If it has comma as decimal separator?
                         # Our script outputs '.', assuming standard python float str.
                         # But if locale mixed in...
                         return pd.to_numeric(x.replace(',', '.').replace(' ', ''), errors='coerce')
                     return pd.to_numeric(x, errors='coerce')
                 
                 cols_to_fix = ['Belopp', 'Utl. belopp']
                 for col in cols_to_fix:
                     if col in df.columns:
                         df[col] = df[col].apply(clean_amount).astype(float)
             
             return df

        df_new = clean_df(df_new)
        
        # Determine source for merging
        merge_file = merge_source if merge_source else (output_path if os.path.exists(output_path) else None)
        
        if merge_file:
            try:
                print(f"Merging with existing history from '{merge_file}'...")
                # Read existing CSV
                # separator is ';'
                # Header is on row 4 usually in the output (lines 0,1,2 are metadata)
                df_old = pd.read_csv(merge_file, sep=';', skiprows=3, encoding='utf-8')
                
                df_old = clean_df(df_old)
                
                # Concatenate
                df_final = pd.concat([df_new, df_old])
                
                # Drop duplicates
                df_final = df_final.drop_duplicates()
                
            except Exception as e:
                print(f"Warning: Could not read file '{merge_file}' for merging: {e}")
                print("Proceeding with new data only.")
                df_final = df_new
        else:
            df_final = df_new

        
        # Sort by Datum descending
        if 'Datum' in df_final.columns:
             df_final = df_final.sort_values(by='Datum', ascending=False)

        # Safe Atomic Update Logic
        temp_output_path = output_path + ".tmp"
        
        # Write to temp file first
        # We need to construct the file with the metadata header.
        # We'll use the metadata from the NEW file (the XLSX) since that's the latest export.
        
        # Read the raw excel again just to get lines 0-2 (metadata)
        df_meta = pd.read_excel(input_path, header=None, nrows=3, engine='openpyxl')
        
        with open(temp_output_path, 'w', encoding='utf-8') as f:
            # 1. Write the 3 metadata rows from df_meta
            for i in range(len(df_meta)):
                # Cleanup line 3 (index 2) to avoid "Totalt övriga händelser"
                row_vals = df_meta.iloc[i].fillna('').astype(str).tolist()
                
                # If any cell contains "Totalt övriga", blank it out or replace
                cleaned_row = []
                for val in row_vals:
                    if "Totalt övriga" in val:
                        cleaned_row.append("") # Replace with empty
                    else:
                        cleaned_row.append(val)
                
                # join with ';' and write
                line = ';'.join(cleaned_row)
                f.write(line + '\n')
            
            # 2. Write the df_final with headers
            df_final.to_csv(f, sep=';', index=False, encoding='utf-8')
            
        # Verification & Swap
        old_count = 0
        if os.path.exists(output_path):
             try:
                 df_existing_check = pd.read_csv(output_path, sep=';', skiprows=3, encoding='utf-8')
                 old_count = len(df_existing_check)
             except:
                 pass # Might be corrupted or empty, proceed safely

        new_count = len(df_final)
        
        # Safety Check: Do not overwrite if we lost data coverage
        # Exception: explicit merge_source might mean we are intentionally building a new file, 
        # but if we are targeting an EXISTING file, we expect growth.
        if os.path.exists(output_path) and new_count < old_count:
             print(f"CRITICAL ERROR: New file has fewer rows ({new_count}) than existing file ({old_count}). Aborting safe update.")
             if os.path.exists(temp_output_path):
                 os.remove(temp_output_path)
             sys.exit(1)
             
        # Perform Swap
        if os.path.exists(output_path):
            backup_path = output_path + ".bak"
            if os.path.exists(backup_path):
                os.remove(backup_path) # Rotate old backup
            os.rename(output_path, backup_path)
            print(f"Backed up existing file to '{backup_path}'")
            
        os.rename(temp_output_path, output_path)
        print(f"Successfully converted '{input_path}' to '{output_path}' (merged {len(df_final)} records)")
        
    except Exception as e:
        print(f"Error converting file: {e}")
        sys.exit(1)


def validate_csv(file_path):
    print(f"\nValidating '{file_path}'...")
    try:
        df = pd.read_csv(file_path, sep=';', skiprows=3, encoding='utf-8')
        
        is_valid = True
        
        # Check for duplicates
        dupes = df[df.duplicated()]
        if not dupes.empty:
            print(f"WARNING: Found {len(dupes)} duplicate rows!")
            print(dupes)
            is_valid = False
        else:
            print("No duplicate rows found.")
            
        # Check sorting
        if 'Datum' in df.columns:
            dates = pd.to_datetime(df['Datum'], errors='coerce')
            if dates.is_monotonic_decreasing:
                 print("Sorting: OK (Descending)")
            else:
                 print("WARNING: Data is not strictly sorted descending by Date.")
                 # We might not want to fail validation just for sorting if it's not critical, 
                 # but usually it is for this app. Let's keep it as a warning but maybe not delete backup?
                 # tailored to user request: "confirmed that no old data is from the conversion"
                 # Deduplication is the main "data integrity" check for "no old data is lost" (or rather, no duplicates created).
                 # Let's be strict.
                 is_valid = False
        
        print("Validation complete.")
        return is_valid
        
    except Exception as e:
        print(f"Validation failed with error: {e}")
        return False

import argparse

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Convert XLSX transaction export to CSV.")
    parser.add_argument("input_file", help="Path to the input XLSX file")
    parser.add_argument("output_file", help="Path to the output CSV file")
    parser.add_argument("--merge-source", help="Path to an existing CSV file to merge history from", default=None)
    parser.add_argument("--no-validate", action="store_true", help="Skip validation steps")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input_file):
        print(f"Error: Input file '{args.input_file}' not found.")
        sys.exit(1)

    convert_xlsx_to_csv(args.input_file, args.output_file, merge_source=args.merge_source)
    
    if not args.no_validate:
        if validate_csv(args.output_file):
            # Validation passed, remove backup
            backup_path = args.output_file + ".bak"
            if os.path.exists(backup_path):
                os.remove(backup_path)
                print(f"Validation passed. Backup file '{backup_path}' deleted.")
        else:
            print("Validation failed (issues found). Backup file preserved.")
            sys.exit(1)
